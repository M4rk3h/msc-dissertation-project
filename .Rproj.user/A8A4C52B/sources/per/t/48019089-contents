---
title: "NHS Wales Data"
author: "Mark Baber"
date: "2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = FALSE
)
```

# Web Scraping NHS Wales

This report will go over the steps taken to web scrape data from an NHS Wales website. 
This involves:

- Downloading the files into rstudio
- Carrying out some EDA
- Plotting the data
- Feature Extraction/Creation

## 0 - Set a Working Directory
Usually when working within R, I start by creating a new folder and a project file, this can be done via the gui (top right of rstudio) or via the console.

![Create Project Via RStudio](img/create-project.png)


```{r}
getwd()
```

Whilst I often switch between operating systems (Linux & Windows) this isn't always an ideal way of setting the project, especially when using online cloud storage. This works much better via Git && Github.

## 1 - EDA
Firstly, when working with scripts and data science coding, it is usually very helpful to set the seed, for reproducibility.

For the seed I set it as my student number from USW - but can be anything.
```{r set-seed}
# 17076749
set.seed(17076749)
```

Next would be to set the libraries which could be useful throughout this report, most jobs within r can be done with the base set libraries, but sometimes it's fun to explore other methods which make some tasks a lot easier/faster. These packages aren't set in stone, and are dynamic for this report.

```{r install-libraries, message=FALSE, warning=FALSE, include=FALSE}
library(funModeling)
library(ggplot2)
library(readxl)
library(tidyverse)
library(dplyr)
```

Next would be to import the data, this part proved to be quite difficult as I kept getting different errors when trying to download the files locally. I will add the code just in case I find something which sparks an idea.


## 1 - Meta Data

[GP Practice Analysis and Patient Registrations by Practice](https://nwssp.nhs.wales/ourservices/primary-care-services/general-information/data-and-publications/gp-practice-analysis/)

# they keep a note of tranfers too.

> The data shows the number of items prescribed by each practice by month and the number of patients registered with each practice. 

> Data relating to patient registrations by practice is extracted from NHAIS (National Health Application and Infrastructure Services) system each quarter. 

> The Health Board, practice code, postcode and count by age band and gender are included. Please note that patients 95 and over have been grouped together due to potential risk of disclosure.

With a better idea of the data after looking at the meta data above, this will give us a better idea of what to look for within the dataset - and a good reminder for us throughout the report.


```{r download-files-via-rstudio, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# set the url to test
# url <- "https://nwssp.nhs.wales/ourservices/primary-care-services/primary-care-services-documents/gp-practice-analysis-docs/patient-registrations-july-2021" 
# notice it doesn't end with .xlsx or .xls

# set a destination of where to save the file (with extension?)
# destfile <- "data/nhs-data/testfile"

# try to download the file with built in download.file
# download.file(url,      # what to save
#              destfile, # where to save
#              mode="wb")# wb convert it to 2 cols

# After some more research, I found some documentation saying about how 
# difficult using XLSX files can be within R.

# browseURL('http://j.mp/2aFZUrJ')

# found a package rio 'R, Input, Output'
# will try to use this for the XLSX files.
# library(rio)
# import(url, format = "xlsx")
# rio still didn't fix the issue.

#rm(df)
#df <- read_excel(path = "data/nhs-data/testfile")
#head(df)
```

The above snippet (which is all commented out and won't be be printed) looks like it worked, however when I try to import the data and have a look at it, the data has been squashed together into 2 columns - My thought is that the file is failing to download correctly, but I'm unsure why (as I can download the package via the command line (Bash) or by clicking on it.)

This is the best I could do whilst trying to download a file from the NHS Wales website, this was done with the extension '.xlsx' and without.  So I will download them **manually** by just clicking the links on the website and saving them to a data folder within my project.

![Project Folder Layout](img/layout.png "Via Windows 11")

After creating a data folder and an _img_ folder (for screenshots), now we can read the saved datasets from that data folder with read_excel.  Doing them all at once is quite demanding, but it's not too bad.

```{r include=FALSE}
df.april20 <- read_excel("data/nhs-data/2020-04.xlsx", sheet = "Sheet1")
df.july20 <- read_excel("data/nhs-data/2020-07.xlsx", sheet = "Sheet1")
df.october20 <- read_excel("data/nhs-data/2020-10.xlsx", sheet = "Sheet1")
df.jan21 <- read_excel("data/nhs-data/2021-01.xlsx", sheet = "Sheet1")
df.april21 <- read_excel("data/nhs-data/2021-04.xlsx", sheet = "Sheet1")
df.july21 <- read_excel("data/nhs-data/2021-07.xlsx", sheet = "Sheet1")
```

Now that we have created 6 data frames each containing a month of GP visits, these are taken quarterly by the look at:
- Jan
- April
- July
- October

## 2 - Data Wrangling

[Data Wrangling](https://xseedcap.com/news-article/big-data-scientists-janitor-work-key-hurdle-insights/) - Accessed 01/08/2021

Data Wrangling has been described as the Janitor Work for data scientists, and is said to take up to 50 - 80 percent of any data scientists time (Ref). With this being the case, this part of the report could become quite large and possibly overwhelming. To over come this, lets break down each part and focus on one stage at a time.  

Here we now have 6 datasets from the same source, whilst these 6 datasets could be explored one at a time, lets try to combine these datasets and explore the data as a whole. First we would need to inspect the datasets to make sure they're following the same structure throughout. An easy way to check the columns is by using the function *names*.

```{r}
print(names(df.april20))
print(names(df.july20))
print(names(df.october20))
print(names(df.jan21))
print(names(df.april21))
print(names(df.july21))
```

Here we can see that the last 2 data frames have an additional column, _HAName_. From what I can tell, this is the Local Health Board which is also denoted by the OrgCode - To make things easier, lets grab the Local Health Boards, figure out which one refers to which code, and replace the code with the LHB.

To begin, lets create a data frame and get the unique Health Boards.

```{r}
# create a data frame.
df <- data.frame()
# get unique Health Boards
uHBs <- unique(df.april21$HAName)
# print LHBs
print(uHBs)
```

Now we can see the LHBs, we can use these to filter one of the newer data frames which has the HAName/LHB.

```{r}
# Filter the last data frame with the unique health board
# as a new data frame
my.AB <-  filter(df.july21, HAName == uHBs[1])  # Aneurin Bevan
my.BC <-  filter(df.july21, HAName == uHBs[2])  # Betsi Cadwaladr Uni
my.HD <-  filter(df.july21, HAName == uHBs[3])  # Hywel Dda
my.CV <-  filter(df.july21, HAName == uHBs[4])  # Cardiff And Vale Uni
my.CTM <- filter(df.july21, HAName == uHBs[5]) # Cwm Taf Morgannwg UHB
my.SB <-  filter(df.july21, HAName == uHBs[6])  # Swansea Bay UHB
my.PT <-  filter(df.july21, HAName == uHBs[7])  # Powys Teaching
```

Above we have filtered out the LHBs from the latest data frame, next we can check each filtered data frame for the OrgCode.

```{r}
# check if they all have a unique OrgCode
print(paste('AB   =',  unique(my.AB$OrgCode)))
print(paste('BC   =',  unique(my.BC$OrgCode)))
print(paste('HD   =',  unique(my.HD$OrgCode)))
print(paste('CV   =',  unique(my.CV$OrgCode)))
print(paste('CTM  =', unique(my.CTM$OrgCode)))
print(paste('SB   =',  unique(my.SB$OrgCode)))
print(paste('PT   =',  unique(my.PT$OrgCode)))
```

Now we know:
- 7A1 - Betsi Cadwaladr Uni
- 7A2 - Hywel Dda
- 7A3 - Swansea Bay UHB
- 7A4 - Cardiff And Vale Uni
- 7A5 - Cwm Taf Morgannwg UHB
- 7A6 - Aneurin Bevan
- 7A7 - Powys Teaching

Now that we have the HBs saved and filtered, we can drop those columns from the newer datasets, combine the datasets and then add the HBs back in (in place of the OrgCode). This is a bit over the top, and there is bound to be a better way of doing things.

```{r}
# DROP HAName
df.april21 <- df.april21[-5]
df.july21 <- df.july21[-5]
```

Here we have dropped the 5th column which is HAName. Next lets look at getting the column names from the older datasets (double check they're the same for the others) and make sure all column names are the same. 
_(I noticed the modern datasets didn't have the same capitalization on the columns, hence the next steps.)_

```{r}
# get col names from a dataset from 2020
colNames <- names(df.april20)

# rename colnames with the same as older datasets
colnames(df.april21) <- colNames
colnames(df.july21) <- colNames
```

Before combining the datasets into something massive, lets try and create a new column for year and month (This was very difficult doing it within a loop on the joined dataset.)

```{r}
df.april20$Year <- 2020
df.april20$Month <- "April"

df.july20$Year <- 2020
df.july20$Month <- "July"

df.october20$Year <- 2020
df.october20$Month <- "October"

df.jan21$Year <- 2021
df.jan21$Month <- "January"

df.april21$Year <- 2021
df.april21$Month <- "April"

df.july21$Year <- 2021
df.july21$Month <- "July"

```

With the Year and Month cols created (and much easier than trying to run it in a conditional loop), we can merge the dfs with rbind and drop the period col.

```{r}
# Bind the datasets with rbind.
# ?rbind
df <- rbind(df.april20, df.july20, df.october20,
                df.jan21, df.april21, df.july21)
# noticed the dataset added a '.'
# drop the period
df <- df[-1]

#df
#df2020 <- df %>% filter(Year == 2020)
```

Here we can see that the data frame was created without any issues. Lets replace the column OrgCode for the Health Board - and rename the column name afterwards. 

```{r}
# Replace OrgCode for LHBs.
df["OrgCode"][df["OrgCode"] == "7A1"] <- "Betsi Cadwaladr Uni"
df["OrgCode"][df["OrgCode"] == "7A2"] <- "Hywel Dda"
df["OrgCode"][df["OrgCode"] == "7A3"] <- "Swansea Bay UHB"
df["OrgCode"][df["OrgCode"] == "7A4"] <- "Cardiff And Vale Uni"
df["OrgCode"][df["OrgCode"] == "7A5"] <- "Cwm Taf Morgannwg UHB"
df["OrgCode"][df["OrgCode"] == "7A6"] <- "Aneurin Bevan"
df["OrgCode"][df["OrgCode"] == "7A7"] <- "Powys Teaching"
# Sanity check OrgCodes.
df$OrgCode %>% unique()
```

Lets quickly rename OrgCodes to Health Board so it is a bit easier to understand.

```{r}
# rename column 3
names(df)[3] = "HealthBoard"
# double check the column names
print(names(df))
```

Great - now we have a combined dataset (More datasets could be added to make things better, for example lsoa which will hopefully be explored later.) lets remove the older data frames to clear up some memory.

```{r}
# remove original datasets
remove(df.april20, df.july20, 
       df.october20, df.jan21, 
       df.april21, df.july21)
# remove filtered datasets
remove(my.AB, my.BC, my.HD, my.CV, 
       my.CTM, my.SB,my.PT)
# the other values don't need to be removed yet.
print(df$Year %>% unique())

# the other values don't need to be removed yet.
print(df$Month %>% unique())
```


This next part will be omitted

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Omitted
# Before moving on, lets try to add on more dataset which could help pinpoint where in each LSOA a GP is, this can be done with the datasets from the same website.

# [GP Practice Analysis Data](https://nwssp.nhs.wales/ourservices/primary-care-services/general-information/data-and-publications/gp-practice-analysis/)

# First, make a copy of the original df.
```



```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# make a copy
# dfC <- df
gpPA <- read_xlsx("data/gp-practice-analysis-2020.xlsx")
# gpPA %>% head()
# rename PracticeID to PracticeCode
colnames(gpPA)[colnames(gpPA) == "PracticeID"] <- "PracticeCode"
# rename PracticeID to PracticeCode
colnames(gpPA)[colnames(gpPA) == "Postcode"] <- "PostCode"
# now merge them by PracticeCode & Postcode
# dfC <- merge(dfC, gpPA, by = c("PracticeCode","PostCode"))
```


This data frame could also be cleaner, to create tidy data we need 3 things:

1 - Every column is a variable
2 - Every row is an observation
3 - Every cell is a single value

This was adapted from the [R for Data Science](https://r4ds.had.co.nz/index.html).
This is where I got the idea for Year and Month to be separate, but did it before the binding of the data frames above. Seeing as that part was done, lets begin some actual EDA.

## 3 - Exploratory Data Analysis

Lets now go through the dataset and get a better understanding of the structure, the data, and just explore the data.

```{r}
str(df)
```

From the structure (str) we can see that there are 9 variables with 232,292 inputs, these variables are as follows:

- Practice Code - The GPs practice code.
- PostCode - The GPs postcode.
- HealthBoard - The GPs health board (Aneurin Bevan, Cardiff etc)
- AgeBand - The patients age.
- MaleCount - The number of male visits.
- FemaleCount - the number of female visits.
- IndeterminateCount - the number of intersex visits.
- Count - The total number of visits (males, female, intersex maybe).
- Year - The year.
- Month - The month.

We can also look at the data via the head and tail.

```{r}
head(df)
tail(df)
```

Along with the summary.

```{r}
summary(df)
```

Summary is a very useful function as we can easily see the _min, median, mean, max_ along with some quartiles - and most importantly we can see which columns have NA's.. I'm looking at you **AgeBand**. 2365 NA's is quite a lot, so this would need to be dealt with as best as possible (this could be dropping all the values, even if it is quite a lot - or simply putting the age band as an outlier so we can easily spot them.)

```{r}
paste("There are", (df$AgeBand %>% is.na() %>% sum()), "Missing values")
# with a count of 2365 - summary was correct.
```

Even when looking at the unique values of the age, we can see that NA is in there. This could be because of a number of reasons, but there are 2 assumptions which stand out to me:

- Someone refused to give their age -- which is fine .
- They were over 94 which was stated in meta data to be grouped.

Lets now look at the unique values within each column.

```{r}
uniqueVals <- vector()
i = 0
for (i in 1:length(df)) {
  uniqueVals[i] <- 
    paste(
    length(
      unique(
        unlist(df[i]))), "unique values in", names(df[i]))
  
} 
# print the unique values for each column.
print(uniqueVals)
```

Before moving on, it's important to do something with the missing values within the AgeBand - this can be done by replacing the values with the mean value or just changing them to 0's. Here, lets change them to the mean value.

```{r}
df$AgeBand[is.na(df$AgeBand)] <- mean(df$AgeBand, na.rm = TRUE)
```

Whilst some of the code above didn't really show anything which stood out (mainly the unique counts) - there are many packages which I found when researching EDA in R with this one below being _skimr_ which is great for a Data Summary.

```{r}
# https://www.rdocumentation.org/packages/skimr/versions/2.1.3
library(skimr)

df %>% skim()
```
The skim package is great for explaining a quick overview of the data, with 4 character variables and 6 numeric variables, the mean, sd, quantiles and a small histogram of each numerical dataset.

We can also try to uncover some descriptive statistics using the package funModeling - this adds a few fun functions which show off the data in a different way again (similar to the above package.)

```{r}
# we can produce some descriptive stats with profiling_num.
profiling_num(df)
# print to check the status of the data frame
status(df)
```

As we saw with the 2 functions (profiling and status) the dataset can be seen as a whole which sometimes can be overwhelming, especially within data science but will make going through the data a lot easier in the long run. 

So the next part demonstrates how to get some descriptive statistics from the base r packages.

```{r}
paste("The mean malecount (visits) is", 
      mean(df$MaleCount) %>%
        round(2))
paste("The mean femalecount (visits) is",
      mean(df$FemaleCount)  %>%
        round(2))
paste("The mean count (visits) is",
      mean(df$Count) %>%
        round(2))
```

Next lets take a quick look at the average number of visits per health board.

```{r}
avg.lhb <- aggregate(formula = Count ~ HealthBoard,
                     data = df,
                     FUN = mean)
# order it asc
avg.lhb[order(avg.lhb$Count),]
```

From the above we can see that Cwm Taf had the highest mean count compared to the other LHBs - which is interesting as from looking at the _Population Estimates_ per LHB, Cwm Taf has the 4th largest _Population estimate_ which would suggest a different characteristic is affect their visits, for example:

- Is there a lot of industry in Cwm Taf areas?

- [Is there a higher rate of deprivation?](https://statswales.gov.wales/Catalogue/Community-Safety-and-Social-Inclusion/Welsh-Index-of-Multiple-Deprivation/WIMD-2019/localhealthboardanalysis)

- How is the Population Health of Cwm Taf?

- [Level of education](https://statswales.gov.wales/Catalogue/Education-and-Skills/Post-16-Education-and-Training/Data-For-Regions-of-Wales/highestqualificationlevelofworkingageadults-by-region-localauthority) - Highest gained qualification

This was a very quick look at the data - next we want to try and pull out some more insights from this dataset. This can be done by looking at the percentages of the data and trying to plot it for a better picture.

## 4 - Plots

This section will look to explore the dataset and produce some plots - this will use a range of packages from base r packages (plot, barplot etc), ggplot, funModeling and more.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# frequency looks at plotting some of the numerical data
freq(df)
```

The freq function from the funModeling package is great to quickly look at the frequencies in the dataset and creates some plots for us - with the Health Board Frequency plot looking very interesting.

```{r}
# numerical profiling in one function 
# automatically excludes non-numerical variables
plot_num(df)

```

plot_num plots all of the numerical values within a dataset and plots them nicely, although this plot isn't too interesting to us in this form. 

Lets try to look at the average visits per GP.

```{r}
lhbs <- avg.lhb$HealthBoard

plot(x = avg.lhb$Count,
     main = "Avg Age seen per HealthBoard",
     xlab = "Local Health Board",
     ylab = "Average Age",
     type = "b")

pp <- ggplot(avg.lhb, 
             aes(Count, 
                 HealthBoard,
                 colour = HealthBoard)) +
  geom_point(aes(size = 6)) + 
  labs(title = "Average Age Seen Per Health Board", 
       subtitle = "From 2020 - 2021")
pp
```

For now, lets focus on Aneurin Bevan which covers a big area (and he's the guy who started the NHS). If we can filter the dataset, perhaps we can start to find some more variables to add on to the dataset. Specifically adding lsoa, Deprivation levels whilst looking at housing variables for the same areas. Some of these things might be out of the scope of this project (due to time and ability) which is a shame, but could open up the possibilities for future research.

Lets filter the data frame for just AB, and change the column names to lowercase (personal preference.)

```{r}
# filter the df by lhb
df.ab <- filter(df, HealthBoard == "Aneurin Bevan")

# change col names to lower.
names(df.ab) <-    # replace names
  names(df.ab) %>% # with the same names
  tolower()     # but lowercase

# check structure
str(df.ab)
```

After checking the structure to get a better feel for the dataset, this can be done by creating a function of a few of the eda we covered above. This function was found from a Pablo Casas' [Data Science Live Book - 2019](https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/) - Accessed 05/08/2021.

```{r}


# function for basic eda
basic_eda <- function(data)
{
  glimpse(data)
  print(status(data))
  freq(data) 
  print(profiling_num(data))
  plot_num(data)
  describe(data)
}

```

Above we created a basic EDA function using the a few functions from Tidyverse & Funmodeling.
```{r}
basic_eda(df.ab)
```

The basic eda shows a lot about the data, especially with the frequency and percentages the data makes up. Lets focus on getting the LSOAs added.

```{r}
freq(df.ab$postcode,
     plot = F)
```

After looking at _freq_ we can see that the top 5 postcodes are around 2% which is higher than the next 5 by 0.5+ - this is where looking at the lsoa and variables for an area.

Aneurin Bevan HB covers 5 local authorities:

- Caerphilly
- Newport
- Torfaen
- Blaenau Gwent
- Monmouthshire


```{r}
# now to map the postcodes to lsoas
# check unique post codes
df.ab$postcode %>% 
  unique()

lsoa <- 
  data.frame()

u.postcodes <- 
  df.ab$postcode %>% 
  unique()

u.postcodes %>% 
  summary()
# 73 entries.

# write.csv(u.postcodes, file = "data/nhs-data/post-codes.csv")
```

After getting all of the unique postcodes within AB, it was time to try and find the lsoa's. For this I found a lsoa reference from ONS, which was great (apart from the file being around 1.5gb when being opened and around 2 **million** records). This made using software like Excel, Notepad, and VSCode to try and open the file, a no go. 

I could open the file within Notepad ++ (Which I respect a lot now), but in the end I tried to do it within a plain rstudio r script, this proved to be too big to use within RStudio Cloud which is what I was using at the time of writing this script, so I did this locally. With there being 73 entries which we've managed to pull out into a variable, you'd think I could create some clean script with use s/l/Apply or something. But this was the only way I thought to do it:

Get the dataset from the internet

```{r}
# library(readr)
# lsoa_dataset <- read_csv("https://www.arcgis.com/sharing/rest/content/items/ce3b53cb8a6c4fee93e5d560d7b1123d/data")
# obviously this doesn't work.
```

Download the dataset, locally outside of RStudio and import it into RStudio separately. 

```{r}
# This is a big file.
lsoa.df <- read.csv("data/nhs-data/lsoa-dataset.csv")

# check the head
lsoa.df %>% 
  head()
# File is around 1GB when opened
```
Seeing as the file is so big, this needs to be done locally instead of RStudio cloud! We also only need a few of the columns. One with the postcode, columns 1:3 and lsoa which is column 8, the rest can be deleted. (Whilst this might not free up some memory, it will make things easier for us.)

```{r}
lsoa.df <- 
  lsoa.df[c(3,8)]
```


```{r}
lsoa.df %>%
  str()

lsoa.df %>%
  summary()
```

Now to filter the data frame to find the lsoa's for the postcodes we have/want. Lets try to loop through the 2 different datasets, to create 1 new dataset with the lsoa codes, that match up to our postcodes.

```{r}
# create a variable for our postcodes/lsoa codes
lsoaCodes <- vector()
# set i to 0
i = 0
# loop through for the length of unique postcodes
for (i in 1:length(u.postcodes)) {
  lsoaCodes[i] <- lsoa.df$lsoa11cd[lsoa.df$pcds == u.postcodes[i]]
}
print("Loop finished.")
# insert into lsoaCodes at entry 0 (i),
# the entry from lsoa.df$lsoa where the postcode == that of the 
# postcode from the unique postcodes. 
# This is stupid but it worked =P
```

After filtering the lsoa data frame (2 million records) and only keeping those which match our unique postcodes (lets remove this dataset from rstudio as it's around 1GB), we can assume that it did it in the order of our unique postcodes. This would suggest that we can just connect our unique postcodes as a new column within the data frame. First lets check the data frame to make sure it didn't get changed in the loop.

```{r}
# remove the big dataset
remove(lsoa.df)
# check the class
paste("The class of lsoaCodes is", class(lsoaCodes), "with a summary below.")
# check the summary
lsoaCodes %>% 
  summary()
```

We can easily change the class back to data frame with as.data.frame.

```{r}
lsoaCodes <- as.data.frame(lsoaCodes)
# rename the column to lsoa
names(lsoaCodes) <- "lsoa"
# check the class again
paste("The class of lsoaCodes is now", class(lsoaCodes))
# check the head.
lsoaCodes %>% head()
```
Now we want to add the unique postcodes to a new column in lsoaCodes

```{r}
# join the postcodes (yolo it)
lsoaCodes <- as.data.frame(lsoaCodes)

lsoaCodes$postcode <- 
  u.postcodes
# check they joined correctly.
lsoaCodes %>%
  head()
# now to sanity check this, I went through and filtered 1 by 1 and compared to this one.
# write.csv(lsoaCodes, "data/nhs-data/lsoaCodes-check.csv")
```

Now that we have the lsoa and postcodes for all of the LHB AB, we can try to loop through the dataset (df.ab) and input the lsoa to match up with the postcode. But first lets change the names of the data frame to all lower case, this is a personal preference. 

```{r}
# set i as 0
i = 0
# start the loop and loop for as many postcodes _need_ a lsoa.
# there are 2,665,238 records to loop through - best of luck.
for (i in 1:length(df.ab$postcode)) {
  df.ab$lsoa[df.ab$postcode == lsoaCodes$postcode[i]] <- 
    lsoaCodes$lsoa[i]
  # inside the loop, add a column to lsoa which depending 
  # on the first postcode add the first lsoa
}
print("lsoa's done.")
# check unique values of the first 10
df.ab$postcode %>% 
  unique() %>% 
  head(10)

# write it out to check if some postcodes have 2 spaces (NP4 9AW)
# write.csv(df.ab, "data/nhs-data/df.ab.csv")
```

Now to check the df.ab and see if anything is off.

```{r}
class(df.ab$lsoa)
```

This looks like it worked - I finally have a decent data frame. Whilst this is great, we could still benefit from other variables to model the GP Visits against. This should now be easier with the postcodes and lsoa's, as a lot of datasets from stats wales is by lsoa.

First lets run a _basic eda_ on the new dataset and see what stands out.

```{r}
basic_eda(df.ab)
```


It's interesting to see the top 2 LSOAs more than the rest - this could be interesting to go through. 

```{r}
lsoaC <- count(df.ab, 
        lsoa, 
        sort = T)

lsoaC %>%
  head(5)

topTwo <- # save the top two
  c("W01001524", # Pontypool (shocker, I know)
    "W01001586") # West Chepstow!

# https://www.ukcrimestats.com/LSOA/W01001586
```



```{r}
# write out the data frame to end.
#write.csv(df, file = "data/nhs-data/gp-visits-lhb.csv")
#write.csv(df.ab, file = "data/nhs-data/gp-visits-ab.csv")
```



# 3 - Plots ####

This section will go over the datasets gathered thus far and will look to see 

## 3.1 - Age band ####

Whilst we would usually check for normality with the Shapiro Wilk test - within R this is limited to a sample size of 5000 and seeing as this dataset is 43260 the sample size is so large, and this limitation protects us against the risk of the null hypothesis being wrongfully rejected, but also stops us from using this commonly used test here (it might be able to be done in other software like SAS or SPSS). 

Therefore we will visually check the data for normality with plots.

# Write about normality for each numerical col

# Can we compare the number of registered patients to population estimates and look for outliers (Are people traveling for their GP?)

```{r}
# create a function to create a:
# histogram
# density plot?
# boxplot

# pColour <- c("black", "pink", "blue")

# pAge <- ggplot(df.ab, aes(ageband)) + geom_histogram(colour = pColour[1], fill = pColour[2]) + geom_vline(aes(xintercept = mean(ageband)), color = pColour[3], linetype = "dashed", size = 1) + labs(title = toupper(names(df.ab[4])), subtitle = "Source: NHS Wales", x = names(df.ab[4]), y = "count", colour = "Mean")
  

# pAge

hist(df.ab$ageband, 
     main = toupper(names(df.ab[4])),
     xlab = "Age",
     col = "pink")

# Density plot code below
# plot(density(df.ab$ageband), main = toupper(names(df.ab[4])))

boxplot(df.ab$ageband, 
        main = toupper(names(df.ab[4])),
        ylab = "Age",
        col = "pink")
```



```{r}
# library(qqplotr)
#ggplot(df.ab, aes(sample = ageband)) +
#  stat_qq_point(size = 2, col = "blue") +
#  stat_qq_line()
  
#ggplot(gpPA, aes(sample = `Number of Prescribing Patients`)) +
#  stat_qq_point(size = 2, col = "blue") +
#  stat_qq_line()
#hist(gpPA$`Number of Prescribing Patients`)
#qqnorm(gpPA$`Number of Prescribing Patients`)

# Density plot code below
# plot(density(df.ab$ageband), 
#     main = toupper(names(df.ab[4])))
```


```{r}
hist(df.ab$malecount, 
     main = toupper(names(df.ab[5])),
     xlab = "Number of Males seen",
     col = "pink")

boxplot(df.ab$malecount, 
        main = toupper(names(df.ab[5])),
        ylab = "Males seen",
        col = "pink")

# we can make a hist, but it can't show 96 different bars per month - therefor a boxplot is better for looking at normality and the number of ages within the NHS dataset. This is the best we can do without grouping the data in _Actual age bands_ like 0-9, 10-19 etc.

# Atleast a boxplot will show outliers
```


```{r}
hist(df.ab$femalecount, 
     main = toupper(names(df.ab[6])),
     xlab = "Number of Females seen",
     col = "pink")

boxplot(df.ab$femalecount, 
        main = toupper(names(df.ab[6])),
        ylab = "Females seen",
        col = "pink")
```


```{r}
hist(df.ab$count,
     main = toupper(names(df.ab[8])),
     xlab = "Number of patients seen",
     col = "pink")

boxplot(df.ab$count,
        main = toupper(names(df.ab[8])),
        ylab = "Patients seen",
        col = "pink")
```

After roughly plotting all numerical variables within our data, we can see that none of the datasets are 'Normal' - this would be something to be mindful of going forward. Now lets get some more descriptive statistics from this filtered dataset and carry out some statistical analysis.

This can be done by creating a function with the most commonly used functions which describe the dataset.

```{r}
# create my own eda function
eda.2 <- function(data){
  print(mean(data))
  print(sd(data))
  print(var(data))
  print(median(data))
  print(range(data))
  print(quantile(data))
  print(summary(data))
  print(IQR(data))
}
eda.2(df.ab$malecount)
```

Whilst this is great to know how to create our own EDA function, sometimes it's best to just use the summary function, which gives a great _summary_ of the whole data.

```{r}
summary(df.ab)
```



Before moving on to Hypothesis testing and trying to full even more insights from the dataset, lets see if we could get some other datasets to join with this dataset. This could be useful when trying to develop characteristics for the dataset, especially if looking at areas and trying to find patterns. 

```{r}
numOfGPs <- df.ab$practicecode %>% unlist() %>% unique() %>%  length()
paste("There are", numOfGPs, "within", df.ab$healthboard[1]) # paste one
# write.csv(df.ab, file = "data/df.ab.csv")
```

Lets see if we can see the mean number of visits per gp

```{r}
# aggregare the mean number of total visits (count) by lsoa
lsoa.mean <- aggregate(count ~ lsoa, data = df.ab, mean)

# sort the lsoa by count
sort(lsoa.mean$count, decreasing = T) %>% round(2)

# plot it in a hist
hist(lsoa.mean$count,
     main = toupper("Mean number of GP visits per LSOA"),
     xlab = "LSOA",
     col = "pink")
# plot a boxplot
boxplot(lsoa.mean$count,
        main = toupper("Mean number of GP visits per LSOA"),
        xlab = "LSOA",
        col = "pink")
```

After looking at both plots, we can see that there is a few outliers. There's a few things which could be worth exploring:

- Look for just the year 2020 (as it is a full year)
- Break down this by each month we have

```{r}
ab.2020 <- filter(df.ab, year == "2020")
ab.2021 <- filter(df.ab, year == "2021")
# aggregare the mean number of total visits (count) by lsoa
lsoa2020.mean <- aggregate(count ~ lsoa, data = ab.2020, mean)
lsoa2021.mean <- aggregate(count ~ lsoa, data = ab.2021, mean)
# sort the lsoa by count
sort(lsoa2020.mean$count, decreasing = T)
sort(lsoa2021.mean$count, decreasing = T)
# add year
lsoa2020.mean$year <- "2020"
lsoa2021.mean$year <- "2021"
# join them
lsoa.m.means <- rbind(lsoa2020.mean, lsoa2021.mean)
lsoa.m.means <- as.data.frame(lsoa.m.means)
```

```{r}
library(ggplot2)
library("gridExtra")
pp2020 <- ggplot(lsoa2020.mean, aes(x = count)) + 
  geom_histogram(colour = "white", fill = "blue") + labs(x = "Mean visits", y = "Count")

pp2021 <- ggplot(lsoa2021.mean, aes(x = count)) + 
  geom_histogram(colour = "black", fill = "pink") + labs(x = "Mean visits", y = "Count")


grid.arrange(pp2020, pp2021, ncol=1)

ggplot(lsoa2020.mean, aes(count)) + 
   geom_histogram(colour = "white", fill = "blue") + # 2020
   geom_histogram(data = lsoa2021.mean,colour = "black", fill = "pink") + #2021
  labs(x = "Mean visits", y = "Count")
                  
```


It could be interesting to see the the top 5 and see where they are.

# 4 - Hypothesis Testing ####

First lets start by covering some t.tests and see if these could be useful.

```{r}
# normal t.test
t.test(df.ab$ageband,
       df.ab$malecount)

# Need a paired t-test.

# paired t.test
t.test(df.ab$ageband,
       df.ab$malecount,
       paired = T)
# one sample t.test
t.test(df.ab$ageband, mu = 3)
```

Nonparametric tests
Can't run any of these as we don't have a categorical column for the dataset.
```{r}
# Non Numeric Tests
wilcox.test(df.ab$ageband, c(df.ab$malecount,df.ab$femalecount),
            paired = F, alternative = "greater")

wilcox.test(df, y, paired = F, 
            alternative = "greater")
            
# Non Numeric Tests
# wilcox.test(df.ab$ageband, df.ab$malecount, paired = T)
# Can't use this as it required paired datasets

# Kruskal Wallis Test One Way Anova
kruskal.test(df.ab$ageband, df.ab$malecount)



```

Multiple Linear Regression
```{r}
fit <- lm(df.ab$ageband ~ df.ab$malecount)
summary(fit)
```

Other statistical functions
```{r}
coefficients(fit) # model coefficients 
confint(fit, level = 0.95) # CIs for model parameters  
fitted(fit) # predicted values 
residuals(fit) # residuals 
anova(fit) # anova table  
vcov(fit) # covariance matrix for model parameters  
influence(fit) # regression diagnostics
```

Diagnostic plots provide checks for heteroscedasticity, normality, and influential observations. 

```{r}
# diagnostic plots  
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page  
plot(fit) 
```

One Way Anova (Completely Randomized Design) 

```{r}
fit <- aov(df.ab$ageband ~ df.ab$malecount, data = df.ab)

chisq.test(df.ab$ageband)
```


```{r}
# Does gender affect GP visits?
# ppGender <- df.ab %>% group_by(df.ab$lsoa) %>% sum()

# ggplot(aes())

# Does age affect GP visits?

# Does postcode affect GP visits?

df.ab %>% 
  group_by(df.ab$lsoa)

aggLSOA <- aggregate(formula = count ~ lsoa,
          data = df.ab,
          FUN = mean)

# continuous variable ~ categorical variable

ag.lsoa <- aggregate(count ~ lsoa,
          df.ab,
          mean)


ggplot(df.ab) + 
  geom_point(aes(ageband, malecount))


fittie1 <- lm(df.ab$count ~ df.ab$ageband)
fittie2 <- lm(df.ab$count ~ df.ab$malecount)
summary(fittie1)
summary(fittie2)
plot(fittie1)
plot(fittie2)

anova(fittie1, fittie2)

# age_groups <- cut(df.ab$ageband, c(seq(0, 95, by = 5), Inf), include.lowest = TRUE)

ab.nums <- df.ab[-1:-3]
ab.nums <- ab.nums[-7:-8]

res = cor(ab.nums) 
res

library(corrplot)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

# Can we look at demand over time
# Look at plotting the Genders / Age (gender per area)
# Link lsoa to existing datasets
# Can look at Most Popular Visit per Age?
# what proportion of females went out of the whole pop
# Which would be count/sum(malecount) + sum(femalecount)
# Or just for that GP actually
# Which would be count/malecount + femalecount

# Create an empty df.


```


Yeet Me

```{r}
gp.visits <- read.csv("data/nhs-data/gp-visits-ab.csv")
```

