---
title: "NHS Wales Data"
author: "Mark Baber"
date: '2021'
output:
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = TRUE,
	warning = TRUE
)
```

# Web Scraping NHS Wales

This report will go over the steps taken to web scrape data from an NHS Wales website. This involves:

-   Downloading the files into rstudio
-   Carrying out some EDA
-   Plotting the data
-   Feature Extraction/Creation

This report will also try to keep to a Gold Standard, which will be adapted from this [blog post.](https://towardsdatascience.com/the-gold-standard-of-data-science-project-management-13d68c9e85d6)

## 0 - Set a Working Directory

Usually when working within R, I start by creating a new folder and a project file, this can be done via the gui (top right of rstudio) or via the console.

![Create Project Via RStudio](../figures/create-project.png)

Whilst I often switch between operating systems (Linux & Windows) this isn't always an ideal way of setting the project, especially when using online cloud storage. This works much better via Git && Github.

## 1 - EDA

Firstly, when working with scripts and data science coding, it is usually very helpful to set the seed, for reproducibility.

For the seed I set it as my student number from USW - but can be anything.

```{r set-seed}
# 17076749
set.seed(17076749)
```

Next would be to set the libraries which could be useful throughout this report, most jobs within r can be done with the base set libraries, but sometimes it's fun to explore other methods which make some tasks a lot easier/faster. These packages aren't set in stone, and are dynamic for this report.

```{r install-libraries}
library(car)
library(dplyr)
library(funModeling)
library(ggplot2)
library(patchwork)
library(readr)
library(readxl)
library(skimr)
library(tidyverse)
```

Next would be to import the data, this part proved to be quite difficult as I kept getting different errors when trying to download the files locally. I will add the code just in case I find something which sparks an idea.

## 2 - Meta Data

[GP Practice Analysis and Patient Registrations by Practice](https://nwssp.nhs.wales/ourservices/primary-care-services/general-information/data-and-publications/gp-practice-analysis/)

> The data shows the number of items prescribed by each practice by month and the number of patients registered with each practice. Data relating to patient registrations by practice is extracted from NHAIS (National Health Application and Infrastructure Services) system each quarter. The Health Board, practice code, postcode and count by age band and gender are included. Please note that patients 95 and over have been grouped together due to potential risk of disclosure.

-   Whilst also keeping a note of patient transfers.

With a better idea of the data after looking at the meta data above, this will give us a better idea of what to look for within the dataset - and a good reminder for us throughout the report.

```{r download-files-via-rstudio}
# set the url to test
# url <- "https://nwssp.nhs.wales/ourservices/primary-care-services/primary-care-services-documents/gp-practice-analysis-docs/patient-registrations-july-2021" 
# notice it doesn't end with .xlsx or .xls

# set a destination of where to save the file (with extension?)
# destfile <- "../data/data/tidytestfile"

# try to download the file with built in download.file
# download.file(url,      # what to save
#              destfile, # where to save
#              mode="wb")# wb convert it to 2 cols

# After some more research, I found some documentation saying about how 
# difficult using XLSX files can be within R.

# browseURL('http://j.mp/2aFZUrJ')

# found a package rio 'R, Input, Output'
# will try to use this for the XLSX files.
# library(rio)
# import(url, format = "xlsx")
# rio still didn't fix the issue.

#rm(df)
#df <- read_excel(path = "../data/data/tidytestfile")
#head(df)
```

The above snippet (which is all commented out and won't be be printed) looks like it worked, however when I try to import the data and have a look at it, the data has been squashed together into 2 columns - My thought is that the file is failing to download correctly, but I'm unsure why (as I can download the package via the command line (Bash) or by clicking on it.)

This is the best I could do whilst trying to download a file from the NHS Wales website, this was done with the extension '.xlsx' and without. So I will download them **manually** by just clicking the links on the website and saving them to a data folder within my project.

```{r import-data, include=FALSE}
df.april20 <- 
  read_excel("../data/raw/NWSSP Wales AgeGender202004.xlsx",
             sheet = "Sheet1")

df.july20 <- 
  read_excel("../data/raw/NWSSP Wales AgeGender202007.xlsx",
             sheet = "Sheet1")

df.october20 <- 
  read_excel("../data/raw/NWSSP Wales AgeGender202010.xlsx",
             sheet = "Sheet1")

df.jan21 <- 
  read_excel("../data/raw/NWSSP Wales AgeGender202101.xlsx",
             sheet = "Sheet1")

df.april21 <- 
  read_excel("../data/raw/NWSSP Wales AgeGender202104Revised.xlsx", 
             sheet = "Sheet1")

df.july21 <- 
  read_excel("../data/raw/NWSSP Wales AgeGender202107.xlsx",
             sheet = "Sheet1")
```

Now that we have created 6 data frames each containing a month of GP visits, these are taken quarterly by the look at: - Jan - April - July - October

## 2 - Data Wrangling

[Data Wrangling](https://xseedcap.com/news-article/big-data-scientists-janitor-work-key-hurdle-insights/) - Accessed 01/08/2021

Data Wrangling has been described as the Janitor Work for data scientists, and is said to take up to 50 - 80 percent of any data scientists time (Ref). With this being the case, this part of the report could become quite large and possibly overwhelming. To over come this, lets break down each part and focus on one stage at a time.

Here we now have 6 datasets from the same source, whilst these 6 datasets could be explored one at a time, lets try to combine these datasets and explore the data as a whole. First we would need to inspect the datasets to make sure they're following the same structure throughout. An easy way to check the columns is by using the function *names*.

```{r check-column-names}
print(names(df.april20))
print(names(df.july20))
print(names(df.october20))
print(names(df.jan21))
print(names(df.april21))
print(names(df.july21))
```

Here we can see that the last 2 data frames have an additional column, *HAName*. From what I can tell, this is the Local Health Board which is also denoted by the OrgCode - To make things easier, lets grab the Local Health Boards, figure out which one refers to which code, and replace the code with the LHB.

To begin, lets create a data frame and get the unique Health Boards.

```{r check-unique-health-boards}
# create a data frame.
df <- data.frame()
# get unique Health Boards
uHBs <- unique(df.april21$HAName)
# print LHBs
print(uHBs)
```

Now we can see the LHBs, we can use these to filter one of the newer data frames which has the HAName/LHB.

```{r filter-lhbs}
# Filter the last data frame with the unique health board
# as a new data frame
my.AB <-  filter(df.july21, HAName == uHBs[1])  # Aneurin Bevan
my.BC <-  filter(df.july21, HAName == uHBs[2])  # Betsi Cadwaladr Uni
my.HD <-  filter(df.july21, HAName == uHBs[3])  # Hywel Dda
my.CV <-  filter(df.july21, HAName == uHBs[4])  # Cardiff And Vale Uni
my.CTM <- filter(df.july21, HAName == uHBs[5]) # Cwm Taf Morgannwg UHB
my.SB <-  filter(df.july21, HAName == uHBs[6])  # Swansea Bay UHB
my.PT <-  filter(df.july21, HAName == uHBs[7])  # Powys Teaching
```

Above we have filtered out the LHBs from the latest data frame, next we can check each filtered data frame for the OrgCode.

```{r check-orgcode-to-lhbs}
# check if they all have a unique OrgCode
print(paste('AB   =',  unique(my.AB$OrgCode)))
print(paste('BC   =',  unique(my.BC$OrgCode)))
print(paste('HD   =',  unique(my.HD$OrgCode)))
print(paste('CV   =',  unique(my.CV$OrgCode)))
print(paste('CTM  =',  unique(my.CTM$OrgCode)))
print(paste('SB   =',  unique(my.SB$OrgCode)))
print(paste('PT   =',  unique(my.PT$OrgCode)))
```

Now we know: - 7A1 - Betsi Cadwaladr Uni - 7A2 - Hywel Dda - 7A3 - Swansea Bay UHB - 7A4 - Cardiff And Vale Uni - 7A5 - Cwm Taf Morgannwg UHB - 7A6 - Aneurin Bevan - 7A7 - Powys Teaching

Now that we have the HBs saved and filtered, we can drop those columns from the newer datasets, combine the datasets and then add the HBs back in (in place of the OrgCode). This is a bit over the top, and there is bound to be a better way of doing things.

```{r drop-org-code}
# DROP HAName
df.april21 <- df.april21[-5]
df.july21 <- df.july21[-5]
```

Here we have dropped the 5th column which is HAName. Next lets look at getting the column names from the older datasets (double check they're the same for the others) and make sure all column names are the same. *(I noticed the modern datasets didn't have the same capitalization on the columns, hence the next steps.)*

```{r get-col-names}
# get col names from a dataset from 2020
colNames <- names(df.april20)

# rename colnames with the same as older datasets
colnames(df.april21) <- colNames
colnames(df.july21) <- colNames
```

Before combining the datasets into something massive, lets try and create a new column for year and month (This was very difficult doing it within a loop on the joined dataset.)

```{r add-months-and-years}
df.april20$Year <- 2020
df.april20$Month <- "April"

df.july20$Year <- 2020
df.july20$Month <- "July"

df.october20$Year <- 2020
df.october20$Month <- "October"

df.jan21$Year <- 2021
df.jan21$Month <- "January"

df.april21$Year <- 2021
df.april21$Month <- "April"

df.july21$Year <- 2021
df.july21$Month <- "July"

```

With the Year and Month cols created (and much easier than trying to run it in a conditional loop), we can merge the dfs with rbind and drop the period col.

```{r merge-datasets}
# Bind the datasets with rbind.
# ?rbind
df <- rbind(df.april20, df.july20, df.october20,
                df.jan21, df.april21, df.july21)
# noticed the dataset added a '.'
# drop the period
df <- df[-1]

#df
#df2020 <- df %>% filter(Year == 2020)
```

Here we can see that the data frame was created without any issues. Lets replace the column OrgCode for the Health Board - and rename the column name afterwards.

```{r replace-org-with-lhbs}
# Replace OrgCode for LHBs.
df["OrgCode"][df["OrgCode"] == "7A1"] <- "Betsi Cadwaladr Uni"
df["OrgCode"][df["OrgCode"] == "7A2"] <- "Hywel Dda"
df["OrgCode"][df["OrgCode"] == "7A3"] <- "Swansea Bay UHB"
df["OrgCode"][df["OrgCode"] == "7A4"] <- "Cardiff And Vale Uni"
df["OrgCode"][df["OrgCode"] == "7A5"] <- "Cwm Taf Morgannwg UHB"
df["OrgCode"][df["OrgCode"] == "7A6"] <- "Aneurin Bevan"
df["OrgCode"][df["OrgCode"] == "7A7"] <- "Powys Teaching"

# Sanity check OrgCodes.
df$OrgCode %>% unique()
```

Lets quickly rename OrgCodes to Health Board so it is a bit easier to understand.

```{r rename-org-to-lhb}
# rename column 3
names(df)[3] = "healthboard"
# double check the column names
names(df)
```

Great - now we have a combined dataset (More datasets could be added to make things better, for example lsoa which will hopefully be explored later.) lets remove the older data frames to clear up some memory.

```{r clear-some-cache}
# remove original datasets
remove(df.april20, df.july20, df.october20, df.jan21, df.april21, df.july21)

# remove filtered datasets
remove(my.AB, my.BC, my.HD, my.CV, 
       my.CTM, my.SB,my.PT)
# the other values don't need to be removed yet.
df$Year %>% unique()

# the other values don't need to be removed yet.
df$Month %>% unique()
```

This next part will be omitted

```{r tried-to-merge-more-data}
# Omitted
# Before moving on, lets try to add on more dataset which could help pinpoint where in each LSOA a GP is, this can be done with the datasets from the same website.

# [GP Practice Analysis Data](https://nwssp.nhs.wales/ourservices/primary-care-services/general-information/data-and-publications/gp-practice-analysis/)

# First, make a copy of the original df.
```

```{r omitted-code-for-merging}
# make a copy
# dfC <- df
# gpPA <- read_xlsx("../data/gp-practice-analysis-2020.xlsx")
# gpPA %>% head()
# rename PracticeID to PracticeCode
# colnames(gpPA)[colnames(gpPA) == "PracticeID"] <- "PracticeCode"
# rename PracticeID to PracticeCode
# colnames(gpPA)[colnames(gpPA) == "Postcode"] <- "PostCode"
# now merge them by PracticeCode & Postcode
# dfC <- merge(dfC, gpPA, by = c("PracticeCode","PostCode"))
```

Preparing the dataset for additional data, first the dataset needs to follow a few rules which I tend to follow:

-   remove spaces between postcodes (you'll see why soon)
-   all lowercase

```{r remove-spaces-colnames}
# remove spaces from postcode
df$PostCode <- gsub(" ", "", df$PostCode)
# change col names to lower.
names(df) <- names(df) %>% tolower()
```

Next I want to try and add the Welsh Index of Multiple Deprivation - this dataset could be used to get a better understanding of each LSOA. From the dataset I found on [Postcode to WIMD rank lookup](https://statswales.gov.wales/Download/File?fileId=637) the postcodes don't have any white-space, hence why we just removed the white-space above.

I've also noticed that the dataset is formatted as an **ods** file, which can be converted with Excel. After getting the file into an XLSX format, we can import the dataset.

```{r import-wimd-data}
# change file time from ods to xlsx outside of RStudio
# import the WIMD data as wimd.df
wimd_df <- read_excel("../data/raw/postcode-to-wimd-lookup.xlsx", 
    sheet = "Welsh_Postcodes")
# rename Welsh Postcode to postcode
colnames(wimd_df)[colnames(wimd_df) == "Welsh Postcode"] <- "postcode"
# change cols to lower
names(wimd_df) <- tolower(names(wimd_df))
# rename lsoa name (english)
colnames(wimd_df)[colnames(wimd_df) == "lsoa name (english)"] <- "lsoa name"
# remove "2019"  from column names (but make a note of it in plots)
colnames(wimd_df)[colnames(wimd_df) == "wimd 2019 lsoa rank"] <- "wimd lsoa rank"
colnames(wimd_df)[colnames(wimd_df) == "wimd 2019 overall decile"] <- "wimd overall decile"
colnames(wimd_df)[colnames(wimd_df) == "wimd 2019 overall quintile"] <- "wimd overall quintile"
colnames(wimd_df)[colnames(wimd_df) == "wimd 2019 overall quartile"] <- "wimd overall quartile"
# double check it worked. 
names(wimd_df)

wimd_df %>% sample_n(50)
```

```{r merge-wimd-and-df}
df <- inner_join(df, wimd_df, by = "postcode")
# remove spaces
names(df) <- sub(" ", "_", names(df))
# run this twice for some reason... 
names(df) <- sub(" ", "_", names(df))
# sanity check
df %>% head()
```

Next I would like to merge a dataset which found the *County* for the postcodes & LSOA. I've added this with the hopes of adding housing dataset. This was done by looking at the LSOAs found within the dataset and creating an If Else script within Excel to pull out the County per LSOA.

This was from [GOV.Wales](https://gov.wales/docs/statistics/lsoamaps/lsoa.htm) and can be found within "../data/tidy/lsoa-to-county.xlsx" with the the code in the first column which helped a lot.

This dataset has a best fit Area maps for each LSOA within Wales, with 1896 LSOAs in Wales each with an estimates population of around 1500 people.

```{r merge-county-and-df}
county.df <- read_xlsx("../data/tidy/lsoa-to-county.xlsx")
county.df <- county.df[-2]
# change cols to lower
names(county.df) <- tolower(names(county.df))
# now to merge
df <- inner_join(df, county.df, by = "lsoa_code")
# sanity check
df %>% head()
```

Another dataset which could be added now would be *New dwellings started by local authority*. Based on the suggestion that it takes an average of two years to build a house in the UK, I have taken the earliest point in the dataset (April 2020) and backtracked two full years to take a sample from the 'New dwellings started by local authority area and dwelling type' dataset (Stats Wales ref).

This is based on the assumption that houses recorded as being in the process of being built then, would be becoming available on the market approximately April 20202. The alternative was to use the dataset 'New dwellings completed by period and tenure' and measure houses that were available on the market, but this was more difficult as the dataset was not available by county and requires significant effort to combine and clean.

[Link to dataset](https://statswales.gov.wales/Catalogue/Housing/New-House-Building/newdwellingsstarted-by-area-dwellingtype) with the [Link to alternative dataset found here.](https://statswales.gov.wales/Catalogue/Housing/New-House-Building/newdwellingscompleted-by-period-tenure)

```{r import-dwellings-started-dataset}
# import the data
dwell.df <- read_xlsx("../data/raw/new-dwellings-started.xlsx")
# change cols to lower
names(dwell.df) <- tolower(names(dwell.df))
# now to merge
df <- inner_join(df, dwell.df, by = "county")
# sanity check
df %>% head()
# whilst this dataset looks great, lets sort the colnames alphabetically, as I'm very picky.
df <- df %>% 
  select(sort(current_vars()))
# sanity check as I'm tired
df %>% head()
```

Whilst I try to do things efficiently, it is also important to be mindful of tidy data. There are a few rules to tidy data as stated by Hadley Wickham:

1 - Every column is a variable 2 - Every row is an observation 3 - Every cell is a single value

This was adapted from the [R for Data Science](https://r4ds.had.co.nz/index.html). This is where I got the idea for Year and Month to be separate, but did it before the binding of the data frames above. Seeing as that part was done, lets begin some actual EDA.

## 3 - Exploratory Data Analysis

Lets now go through the dataset and get a better understanding of the structure, the data, and just explore the data.

```{r check-str}
# combined df
str(df)
# write.csv(df, file = "../data/nhs-data-full-df.csv")
```

From the structure (str) we can see that there are 15 variables with 321140 observations, these variables are as follows:

-   ageband - the patients age.
-   count - the total number of visits (males, female, intersex maybe).
-   county - the local county.
-   femalecount - the number of female visits.
-   healthboard - the gps health board (aneurin bevan, cardiff etc).
-   indeterminatecount - the number of intersex visits.
-   lsoa_code - the lower layer super output area code.
-   lsoa_name - lower layer super output area.
-   lsoa_name(cymraeg) - lower layer super output area.
-   malecount - the number of male visits.
-   month - the month.
-   postcode - the gps postcode.
-   practice_code - the gps practice code.
-   wimd_lsoa_rank - welsh index of multiple deprivation lsoa rank (lower is worse)
-   wimd_overall_decile
-   wimd_overall_quartile
-   wimd_overall_quintile
-   year - the year.

We can also look at the data via the head and tail.

```{r check-head-tail}
head(df)
tail(df)
```

Along with the summary.

```{r check-summary}
summary(df)
```

Summary is a very useful function as we can easily see the *min, median, mean, max* along with some quartiles - and most importantly we can see which columns have NA's.. I'm looking at you **ageband**. 2365 NA's is quite a lot, so this would need to be dealt with as best as possible (this could be dropping all the values, even if it is quite a lot - or simply putting the age band as an outlier so we can easily spot them.)

```{r print-num-of-nas}
# create an empty vector
missingVals <- vector()
# start i at 0 as we're going to loop.
i = 0
# create the loop
for (i in 1:length(df)) {
  missingVals[i] <- 
    paste("There are", sum(is.na(unlist(df[i]))), "missing values within", names(df[i]))
} 
# print the unique values for each column.
print(missingVals)
```

Even when looking at the unique values of the age, we can see that NA is in there. This could be because of a number of reasons, but there are 2 assumptions which stand out to me:

-   Someone refused to give their age -- which is fine .
-   They were over 94 which was stated in meta data to be grouped.

Lets now look at the unique values within each column.

```{r print-all-unique-values}
# create an empty vector
uniqueVals <- vector()
# start i at 0 as we're going to loop.
i = 0
# create the loop
for (i in 1:length(df)) {
  uniqueVals[i] <- paste("There are", length(unique(unlist(df[i]))), "unique values in", names(df[i]))
} 
# print the unique values for each column.
print(uniqueVals)
```

Before moving on, it's important to do something with the missing values within the ageband - this can be done by replacing the values with the mean value or just changing them to 0's. Here, lets change them to the mean value.

```{r count-nas}
# df$ageband[is.na(df$ageband)] <- mean(df$ageband, na.rm = TRUE) %>% round(0)
# df$ageband[is.na(df$ageband)] <- median(df$ageband, na.rm = TRUE) %>% round(0)
# df$ageband[is.na(df$ageband)] <- -10
# df$ageband[is.na(df$ageband)] <- 0
df <- drop_na(df)
```

Whilst some of the code above didn't really show anything which stood out (mainly the unique counts) - there are many packages which I found when researching EDA in R with this one below being *skimr* which is great for a Data Summary.

```{r use-skimr}
# https://www.rdocumentation.org/packages/skimr/versions/2.1.3
df %>% skim()
```

The skim package is great for explaining a quick overview of the data, with 4 character variables and 6 numeric variables, the mean, sd, quantiles and a small histogram of each numerical dataset.

We can also try to uncover some descriptive statistics using the package funModeling - this adds a few fun functions which show off the data in a different way again (similar to the above package.)

```{r descriptive-stats}
# we can produce some descriptive stats with profiling_num.
profiling_num(df)
# print to check the status of the data frame
status(df)
```

As we saw with the 2 functions (profiling and status) the dataset can be seen as a whole which sometimes can be overwhelming, especially within data science but will make going through the data a lot easier in the long run.

So the next part demonstrates how to get some descriptive statistics from the base r packages.

```{r calc-means-for-all-numerical-vals}
paste("The mean ageband is", mean(df$ageband) %>% round(2))
paste("The mean count is", mean(df$count) %>% round(2))
paste("The mean femalecount is", mean(df$femalecount) %>% round(2))
paste("The mean flats is", mean(df$flats) %>% round(2))
paste("The mean houses is", mean(df$houses) %>% round(2))
paste("The mean malecount is", mean(df$malecount) %>% round(2))
paste("The mean total is", mean(df$total) %>% round(2))
paste("The mean wimd_lsoa_rank is", mean(df$wimd_lsoa_rank) %>% round(2))
paste("The mean wimd_overall_decile is", mean(df$wimd_overall_decile) %>% round(2))
paste("The mean wimd_overall_quartile is", mean(df$wimd_overall_quartile) %>% round(2))
paste("The mean wimd_overall_quintile is", mean(df$wimd_overall_quintile) %>% round(2))
```

Next lets take a quick look at the average number of visits per health board.

```{r avg-visits-per-lhb}
avg.lhb <- aggregate(formula = count ~ healthboard,
                     data = df,
                     FUN = mean)
# order it asc
avg.lhb[order(avg.lhb$count),]

med.lhb <- aggregate(formula = count ~ healthboard,
                     data = df,
                     FUN = median)
# order it asc
med.lhb[order(avg.lhb$count),]
```

From the above we can see that Cwm Taf had the highest mean count compared to the other LHBs - which is interesting as from looking at the *Population Estimates* per LHB, Cwm Taf has the 4th largest *Population estimate* which would suggest a different characteristic is affect their visits, for example:

```{r import-pop-est}
# import pop-est
pop_estimations <- read_csv("../data/web-scraping/pop-estimations.csv")
# delete first col
pop_estimations <- pop_estimations[-1]
# aggregate and find avg
avg.pop <- aggregate(formula = Total ~ `Health Board`,
                     data = pop_estimations,
                     FUN = mean)
# order
avg.pop[order(avg.pop$Total),]
```

-   Is there a lot of industry in Cwm Taf areas?

-   [Is there a higher rate of deprivation?](https://statswales.gov.wales/Catalogue/Community-Safety-and-Social-Inclusion/Welsh-Index-of-Multiple-Deprivation/WIMD-2019/localhealthboardanalysis)

-   How is the Population Health of Cwm Taf?

-   [Level of education](https://statswales.gov.wales/Catalogue/Education-and-Skills/Post-16-Education-and-Training/Data-For-Regions-of-Wales/highestqualificationlevelofworkingageadults-by-region-localauthority) - Highest gained qualification

```{r avg-and-median-wimd}
# average wimd
avg.wimd <- aggregate(formula = count ~ `wimd_lsoa_rank`,
                     data = df,
                     FUN = mean)
# order it asc
avg.wimd[order(avg.wimd$count, decreasing = T),]

med.wimd <- aggregate(formula = count ~ `wimd_lsoa_rank`,
                     data = df,
                     FUN = median)
# order it asc
med.wimd[order(med.wimd$count, decreasing = T),]
```

Above we can see a bit of a difference in the avg wimd index - with a lot of people being around the 160 rank of deprived. Lets have a look at the median WIMD now.

```{r avg-and-median-county}
avg.county <- aggregate(formula = count ~ county,
                     data = df,
                     FUN = mean)
# order it asc
avg.county[order(avg.lhb$count, decreasing = T),]

med.county <- aggregate(formula = count ~ healthboard,
                     data = df,
                     FUN = median)
# order it asc
med.county[order(avg.lhb$count, decreasing = T),]
```

This was a very quick look at the data - next we want to try and pull out some more insights from this dataset. This can be done by looking at the percentages of the data and trying to plot it for a better picture.

## 4 - Plots

This section will look to explore the dataset and produce some plots - this will use a range of packages from base r packages (plot, barplot etc), ggplot, funModeling and more.

```{r check-freq, message=FALSE, warning=FALSE}
# frequency looks at plotting some of the numerical data
freq(df)
```

The freq function from the funModeling package is great to quickly look at the frequencies in the dataset and creates some plots for us - with the Health Board Frequency plot looking very interesting.

```{r plot-numerical-values}
# numerical profiling in one function 
# automatically excludes non-numerical variables
plot_num(df)
```

plot_num plots all of the numerical values within a dataset and plots them nicely, although this plot isn't too interesting to us in this form.

Lets try to look at the average visits per GP.

```{r avg-visits-per-gp}
lhbs <- avg.lhb$healthboard

pp <- ggplot(avg.lhb, aes(count, healthboard, colour = healthboard)) +
  geom_point(aes(size = 6)) + 
  labs(title = "Average Age Seen Per Health Board", 
       subtitle = "From 2020 - 2021")
pp


pp2 <- ggplot()
```

Whilst the plot above shoes a basic average age seen per health board, this isn't the best of plots.

```{r plot-stuff}
# plot the female visits
pp.female_visits <- df %>% 
  group_by(healthboard, ageband) %>%
  summarise(femalecount = sum(femalecount),.groups = 'drop') %>%
  ggplot(aes(x = ageband, y = femalecount, color = healthboard)) +
  geom_line(size = 1.1) + 
  labs(title = str_to_title("Female Count by Ageband and Healthboard"), 
       subtitle = str_to_title("Number of visits per age")) 
# plot the male visits
pp.male_visits <- df %>% 
  group_by(healthboard, ageband) %>%
  summarise(malecount = sum(malecount),.groups = 'drop') %>%
  ggplot(aes(x = ageband, y = malecount, color = healthboard)) +
  geom_line(size = 1.1) + 
  labs(title = str_to_title("Male Count by Ageband and Healthboard"), 
       subtitle = str_to_title("Number of visits per age"))

pp.female_visits
pp.male_visits

pp.female_visits + pp.male_visits + 
  plot_layout(nrow = 2, guides = "collect")
             

# src https://stackoverflow.com/questions/69133366/replicate-excel-plot-with-ggplot2-in-r

# There's a dip around 18-20s - are they going to GPs in other areas
# are they even registering with a new GP if moving to uni?
```

```{r plot-wimd-rank-mean-line}
# plot(df$wimd_lsoa_rank) + abline
```

# 5 - Hypothesis Testing

Before carrying out any Hypothesis tests, usually in ML projects the data would need to be normalised or standardised somehow. From a quick search online, there are two ways to do this within R.

```{r min-max-norm-function}
# Min-Max normalisation
# create a function
min_max_norm <- function(x) {
    (x - min(x)) / (max(x) - min(x))
  }
# https://www.statology.org/how-to-normalize-data-in-r/
```

With the function now created to normalise the data, this could be useful in furute research, especially if moving to ML.


But for now, we will explore the data as it is and start with covering some t.tests and see if these could be useful.

We would first look at the a boxplot to find our first Null Hypothesis.

```{r correlation-test}
# CORRELATION TEST
# check a linear model for rank and count
wimdCountLM <- lm(df$wimd_lsoa_rank ~ df$count)
wimdCountLM$coefficients[1]
# first look at the data in a plot

ggplot(df, aes(wimd_lsoa_rank, count, colour = "pink")) +
         geom_point(alpha = 0.5) + 
  geom_abline(aes(colour = "blue"),
              intercept = wimdCountLM$coefficients[1],
              slope = wimdCountLM$coefficients[2]) +
  labs(title = str_to_title("Scatterplot of wimd lsoa rank and count"),
       subtitle = str_to_title("Number of visits per year"),
       x = str_to_title("wimd rank"),
       y = str_to_title("gp visits")) +
  theme(legend.position = "none")
      

# H0: Deprivation affects GP Visits
# H1: Deprivation doesn't affect GP Visits
cor.wimd.count <- cor(df$wimd_lsoa_rank, df$count, method = "spearman")
cor.wimd.count2 <- cor.test(df$wimd_lsoa_rank, df$count, method = "spearman")
summary(cor.wimd.count2)
# Tiny positive correlation
# My spearmans Rho was 0.001 demonstrating a tiny effect size Cohan, 1988.
# With a P value of 0.53 this relationship was not shown to be significantly significant
```



This section will go over how to carry out a T-test on the _GP Visits modelled by the year_.

```{r t-test}
# T-TEST
ggplot(df, aes(count, year, color = as.factor(year))) +
  geom_boxplot(aes(group = year)) + 
  labs(title = str_to_title("Boxplot of Count and Year"), 
       subtitle = str_to_title("Number of visits per year"),
       x = str_to_title("gp visits"), y = str_to_title("year"),
       color = "Year")
# H0: population variances are equal
leveneTest(df$count ~ as.factor(df$year))
# Use Levene's test to check var.eq
t.test(df$count ~ df$year, var.eq = T) 
```

This next section will cover how to carry out an ANOVA analysis, of the data _GP Visits modelled by County_. 

```{r anova-analysis}
# Make a boxplot of GP visits per county
ggplot(df, aes(count, county, color = as.factor(county))) +
  geom_boxplot() + labs(title = str_to_title("Boxplot of gp visits and County"), 
       subtitle = str_to_title("Number of gp visits per county"),
       x = str_to_title("gp visits"), y = str_to_title("year")) +
  theme(legend.position = "none")

# Carry out an ANOVA for the number of GP visits per county
# mean 
anovaCC <- aov(df$count ~ df$county)
summary(anovaCC)

# can also look at a Tukey test to see 
# difference between all factor means
TukeyHSD(anovaCC) # can also be plotted %>% plot()

# Kruskal Wallis One-way ANOVA
# Used as a sanity check
kruskal.test(df$count ~ df$county)
```

Linear Regression to look at the relationship between number of newly built houses vs the number of GP visits.

```{r linear-regression-model}
lrmPlot <- ggplot(df, aes(total, count, color = as.factor(total))) + 
  geom_point() +
  geom_abline(intercept = 79.43137074,
              slope = 0.05265818) +
  labs(title = str_to_title("Scatterplot of GP Visits against new properties"), 
       subtitle = str_to_title("GP Visits ~ Total number of new properties developed"),
       x = str_to_title("New Properties Developed"), 
       y = str_to_title("GP Visits")) + 
  theme(legend.position = "none")
lrmPlot
# H0: The intercept for our linear model is 0.
# H1: The intercept for our linear model isn't 0.
house.mod <- lm(count ~ total, data = df)
summary(house.mod)

glm(count ~ total, data = df)
```


```{r save-full-data-frame}
# write.csv(df, "../data/tidy/full-gp-visits.csv")
```
